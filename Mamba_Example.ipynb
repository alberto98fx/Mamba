{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsdS2TzdIj-o",
        "outputId": "51ecf025-bdda-4a7b-b796-f1221bc2d720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Collecting mamba_ssm\n",
            "  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Collecting ninja (from mamba_ssm)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (0.8.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (2.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba_ssm) (4.42.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm) (0.19.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: mamba_ssm\n",
            "  Building wheel for mamba_ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba_ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323803485 sha256=cd8ee941b25398d90c135d3a180b5dc33e478c98ae4998b61749809beaff947a\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\n",
            "Successfully built mamba_ssm\n",
            "Installing collected packages: ninja, xxhash, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, mamba_ssm\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 mamba_ssm-2.2.2 multiprocess-0.70.16 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pyarrow-17.0.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub mamba_ssm torch datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://huggingface.co/datasets/alberto98fx/Letteratura-Italiana/resolve/main/full-texts.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYmuL1dwRtwk",
        "outputId": "a66d4091-e31b-4b09-d339-10e676b923e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-09 18:00:08--  https://huggingface.co/datasets/alberto98fx/Letteratura-Italiana/resolve/main/full-texts.txt\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.30.49, 13.33.30.76, 13.33.30.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.30.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/8d/33/8d338a1174ed56cdf26d6f35dde3975d404135392aa367e7c496d94ba3dac472/6c99c9d7a5f8407955ff1cc474f78cfb0cc7fab9129776ef8cc60e0e6428645b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27full-texts.txt%3B+filename%3D%22full-texts.txt%22%3B&response-content-type=text%2Fplain&Expires=1723485608&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzQ4NTYwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzhkLzMzLzhkMzM4YTExNzRlZDU2Y2RmMjZkNmYzNWRkZTM5NzVkNDA0MTM1MzkyYWEzNjdlN2M0OTZkOTRiYTNkYWM0NzIvNmM5OWM5ZDdhNWY4NDA3OTU1ZmYxY2M0NzRmNzhjZmIwY2M3ZmFiOTEyOTc3NmVmOGNjNjBlMGU2NDI4NjQ1Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=FxiV138KpqggCGybbHFkh8yTL0HwebjPJpCkCt6SQe27i1ww0tn2GLH85aEBJ05wJm00iw17TuiNVJOG6Logtj8S3Xs3gj7gW-4Zrbw1wHWp1kZwFLq4NfklY8lQ81RNwBpBgz9aBQb6SnpLoIOVlWP%7EGz1Sy-wVPy9b-RPzkB6LQwKxPZsSy67H9fU-1daNikvJLWHpVkioKiPTriSIQCMk7j48DzdcDFEzb7ZujqYLW8-FHO8Sz5UCfFUkjj8x5M23Emxv5SdFEHgn5c3HzXNaLq%7EAnULA9mZX7PRifABMVjr3Ay9MT-mZZJcgcpMFoer7tQLFu0YkNWwnsgMXBA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-08-09 18:00:08--  https://cdn-lfs-us-1.huggingface.co/repos/8d/33/8d338a1174ed56cdf26d6f35dde3975d404135392aa367e7c496d94ba3dac472/6c99c9d7a5f8407955ff1cc474f78cfb0cc7fab9129776ef8cc60e0e6428645b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27full-texts.txt%3B+filename%3D%22full-texts.txt%22%3B&response-content-type=text%2Fplain&Expires=1723485608&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzQ4NTYwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzhkLzMzLzhkMzM4YTExNzRlZDU2Y2RmMjZkNmYzNWRkZTM5NzVkNDA0MTM1MzkyYWEzNjdlN2M0OTZkOTRiYTNkYWM0NzIvNmM5OWM5ZDdhNWY4NDA3OTU1ZmYxY2M0NzRmNzhjZmIwY2M3ZmFiOTEyOTc3NmVmOGNjNjBlMGU2NDI4NjQ1Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=FxiV138KpqggCGybbHFkh8yTL0HwebjPJpCkCt6SQe27i1ww0tn2GLH85aEBJ05wJm00iw17TuiNVJOG6Logtj8S3Xs3gj7gW-4Zrbw1wHWp1kZwFLq4NfklY8lQ81RNwBpBgz9aBQb6SnpLoIOVlWP%7EGz1Sy-wVPy9b-RPzkB6LQwKxPZsSy67H9fU-1daNikvJLWHpVkioKiPTriSIQCMk7j48DzdcDFEzb7ZujqYLW8-FHO8Sz5UCfFUkjj8x5M23Emxv5SdFEHgn5c3HzXNaLq%7EAnULA9mZX7PRifABMVjr3Ay9MT-mZZJcgcpMFoer7tQLFu0YkNWwnsgMXBA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.165.102.95, 3.165.102.112, 3.165.102.80, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.165.102.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22875127 (22M) [text/plain]\n",
            "Saving to: ‘full-texts.txt’\n",
            "\n",
            "full-texts.txt      100%[===================>]  21.81M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-08-09 18:00:09 (194 MB/s) - ‘full-texts.txt’ saved [22875127/22875127]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://huggingface.co/alberto98fx/Italian-Poet-Mamba/resolve/main/model.pt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFRHqkW8R4d7",
        "outputId": "b5e6cae2-59cd-47cc-b00f-e5f141a35a5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-09 18:00:09--  https://huggingface.co/alberto98fx/Italian-Poet-Mamba/resolve/main/model.pt\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.30.49, 13.33.30.76, 13.33.30.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.30.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/7b/eb/7beba195ca3f73a1354c8053ff59a6ce11c8170c300a30ff7aad91c0bf54c4bd/a81295ac8523da7391567971ac45ceaba2aa9299a258b2d97c96dff8d5d65f36?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.pt%3B+filename%3D%22model.pt%22%3B&Expires=1723484418&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzQ4NDQxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzdiL2ViLzdiZWJhMTk1Y2EzZjczYTEzNTRjODA1M2ZmNTlhNmNlMTFjODE3MGMzMDBhMzBmZjdhYWQ5MWMwYmY1NGM0YmQvYTgxMjk1YWM4NTIzZGE3MzkxNTY3OTcxYWM0NWNlYWJhMmFhOTI5OWEyNThiMmQ5N2M5NmRmZjhkNWQ2NWYzNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=FgE8uwQkOYTup70etwRbQu0S0xk0iwFdXkL0bv1hcHA7POMM60HnhdV-RKOFh30-NzjdSp-dPImxIzrwnsgyW3tiwdGDlHB9T6lTr8VHgDQSqAFDc6Dk0BSGb0Mc3W019jeQxTdZRCHGPePZ9zpj3tvD42OnaiptcMrBM99N1VbnWAkWBrzVZ5fn6tXRJ0lZtJRmAxiEE1bhqmpPLzPNIvs6h9NN4hqceT-wjYETIjwqXb9Bzep9LLAD0wkVelQsxa1jh5c2u52C%7EDB0zJC7OyTddPygaFMbHz1yWsixkX2RjHbiX6GMq0wahQ%7EgnAhFlLhR-VvDRBIpbzYG5U%7E71Q__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-08-09 18:00:09--  https://cdn-lfs-us-1.huggingface.co/repos/7b/eb/7beba195ca3f73a1354c8053ff59a6ce11c8170c300a30ff7aad91c0bf54c4bd/a81295ac8523da7391567971ac45ceaba2aa9299a258b2d97c96dff8d5d65f36?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.pt%3B+filename%3D%22model.pt%22%3B&Expires=1723484418&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzQ4NDQxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzdiL2ViLzdiZWJhMTk1Y2EzZjczYTEzNTRjODA1M2ZmNTlhNmNlMTFjODE3MGMzMDBhMzBmZjdhYWQ5MWMwYmY1NGM0YmQvYTgxMjk1YWM4NTIzZGE3MzkxNTY3OTcxYWM0NWNlYWJhMmFhOTI5OWEyNThiMmQ5N2M5NmRmZjhkNWQ2NWYzNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=FgE8uwQkOYTup70etwRbQu0S0xk0iwFdXkL0bv1hcHA7POMM60HnhdV-RKOFh30-NzjdSp-dPImxIzrwnsgyW3tiwdGDlHB9T6lTr8VHgDQSqAFDc6Dk0BSGb0Mc3W019jeQxTdZRCHGPePZ9zpj3tvD42OnaiptcMrBM99N1VbnWAkWBrzVZ5fn6tXRJ0lZtJRmAxiEE1bhqmpPLzPNIvs6h9NN4hqceT-wjYETIjwqXb9Bzep9LLAD0wkVelQsxa1jh5c2u52C%7EDB0zJC7OyTddPygaFMbHz1yWsixkX2RjHbiX6GMq0wahQ%7EgnAhFlLhR-VvDRBIpbzYG5U%7E71Q__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.165.102.95, 3.165.102.112, 3.165.102.80, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.165.102.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60565050 (58M) [binary/octet-stream]\n",
            "Saving to: ‘model.pt’\n",
            "\n",
            "model.pt            100%[===================>]  57.76M  23.8MB/s    in 2.4s    \n",
            "\n",
            "2024-08-09 18:00:12 (23.8 MB/s) - ‘model.pt’ saved [60565050/60565050]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from mamba_ssm import Mamba\n",
        "\n",
        "# Function to determine vocab size from the model file\n",
        "def get_vocab_size(model_path):\n",
        "    model = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    def find_embedding_layer(model_dict):\n",
        "        for key, value in model_dict.items():\n",
        "            if isinstance(value, torch.Tensor) and len(value.shape) == 2:\n",
        "                if key.endswith('weight') and 'embedding' in key.lower():\n",
        "                    return value\n",
        "        return None\n",
        "\n",
        "    embedding_layer = find_embedding_layer(model)\n",
        "\n",
        "    if embedding_layer is not None:\n",
        "        return embedding_layer.shape[0]\n",
        "    else:\n",
        "        raise ValueError(\"Could not determine vocabulary size. The model structure might be different.\")\n",
        "\n",
        "# Hyperparameters (these should match the ones used during training)\n",
        "block_size = 256\n",
        "n_embed = 384\n",
        "n_heads = 6\n",
        "n_layers = 6\n",
        "dropout = 0.2\n",
        "\n",
        "# Model architecture\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.keys = nn.Linear(n_embed, head_size)\n",
        "        self.queries = nn.Linear(n_embed, head_size)\n",
        "        self.values = nn.Linear(n_embed, head_size)\n",
        "        self.head_size = head_size\n",
        "        self.n_embed = n_embed\n",
        "        self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.keys(x)\n",
        "        q = self.queries(x)\n",
        "        v = self.values(x)\n",
        "        wei = k @ q.transpose(-1, -2) * C ** (-0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = torch.log(torch.exp(wei) + 1)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.gamma = nn.Parameter(torch.ones(dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        xmean = x.mean(dim=1, keepdim=True)\n",
        "        xvar = ((x - xmean) ** 2).mean(dim=1, keepdim=True)\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "        return self.out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(n_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ffn(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_heads):\n",
        "        super().__init__()\n",
        "        self.head_size = n_embed // n_heads\n",
        "        self.sa_head = Mamba(\n",
        "            d_model=n_embed,\n",
        "            d_state=16,\n",
        "            d_conv=4,\n",
        "            expand=2,\n",
        "        )\n",
        "        self.ffn = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa_head(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramNeuralNetwork(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.sa_head = MultiHeadAttention(4, int(n_embed / 4))\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "        self.ffn = FeedForward(n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads=n_heads) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, None\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            last_timestep = logits[:, -1, :]\n",
        "            probs = F.softmax(last_timestep, dim=1)\n",
        "            next_index = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_index), dim=1)\n",
        "        return idx\n",
        "\n",
        "# Determine vocab size and load the model\n",
        "model_path = 'model.pt'\n",
        "vocab_size = get_vocab_size(model_path)\n",
        "print(f\"Determined vocabulary size: {vocab_size}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BigramNeuralNetwork(vocab_size).to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Encoding and decoding functions\n",
        "chars = sorted(list(set(open('full-texts.txt', 'r').read())))\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"Ricordo la farfalla ch’era entrata dai vetri schiusi nella sera fumida su la costa raccolta, dilavata dal trascorrere iroso delle spume.\"\n",
        "encoded_seed = encode(seed_text)\n",
        "seed_tensor = torch.tensor(encoded_seed, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "generated_indices = model.generate(seed_tensor, max_new_tokens=500)[0].tolist()\n",
        "generated_text = decode(generated_indices)\n",
        "\n",
        "print(\"\\nGenerated text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN8zTUOoMSFC",
        "outputId": "23725de8-7f8b-4d7b-bca8-ccb026a3d1b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determined vocabulary size: 151\n",
            "\n",
            "Generated text:\n",
            "Ricordo la farfalla ch’era entrata dai vetri schiusi nella sera fumida su la costa raccolta, dilavata dal trascorrere iroso delle spume.\n",
            "\n",
            "I soliti corsari erano, giunti insieme al notaio, quando videro i mozzi, si videro correre precipitosamente contro le vicinanze dell’avvocato Arzellini, il quale nessuna poteva fare prima del rimorso a Firenze, dirigendosi verso il suo marito.\n",
            "\n",
            "Poi, finita quella prima volta a sua volta, pur sicuro di averlo.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Elegante ordine sapeva a mia moglie un grasso sudicenziario per azione ch’ella aveva consentito al mio rispetto per un maschio e tante parole gli avevano ingiusta veramente. \n"
          ]
        }
      ]
    }
  ]
}